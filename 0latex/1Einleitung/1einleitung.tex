\chapter{Einleitung}

\section{Hintergrund und Motivation}
Die Anwendung von Reinforcement Learning (RL) im Bereich der Finanzmärkte hat in den letzten Jahren erheblich an Bedeutung gewonnen. Insbesondere die Entwicklung von Trading-Bots, die in der Lage sind, autonom Handelsentscheidungen zu treffen, bietet ein enormes Potenzial für die Verbesserung von Handelsstrategien und die Maximierung von Gewinnen. Die Effizienz und Effektivität dieser Systeme hängen jedoch maßgeblich von der Qualität der zugrunde liegenden Modelle und Algorithmen ab.

\section{Problemstellung}
Der Einsatz von RL in Handelsumgebungen bringt spezifische Herausforderungen mit sich. Ein wesentlicher Aspekt ist die Handhabung zeitsequenzieller Daten, bei denen die zeitliche Dimension eine entscheidende Rolle spielt. Ein weiterer zentraler Punkt ist die Gestaltung der Reward-Funktion, die den Lernprozess des Trading-Bots maßgeblich beeinflusst und dessen Fähigkeit zur Erzielung profitabler Handelsentscheidungen bestimmt.

\section{Relevanz und Forschungslage}
Frühere Studien und Anwendungen haben gezeigt, dass RL-Ansätze vielversprechend für die Entwicklung von autonomen Trading-Systemen sind. Arbeiten von Brunton und Kutz sowie Almawlawe et al. haben bereits die Wirksamkeit von RL-Modellen in verschiedenen Kontexten demonstriert \cite{brunton2019data} \cite{Almawlawe2023}. Jedoch bleibt die Herausforderung bestehen, diese Modelle effizient auf handelsübliche Rechner zu übertragen und gleichzeitig ihre Leistungsfähigkeit zu maximieren.

\section{Forschungsmethode und Ansatz}
Diese Arbeit verfolgt den Ansatz, ein RL-Modell für Trading-Bots zu entwickeln, das auf einem handelsüblichen Rechner mit begrenzten Ressourcen trainiert werden kann. Hierbei wird ein spezieller Konvolutions-Layer eingesetzt, der es ermöglicht, sehr lange Datenreihen zu verarbeiten, ohne die Qualität der Ergebnisse zu beeinträchtigen. Zudem wird eine logarithmische Funktion verwendet, um die Verarbeitung der Daten im Konvolutions-Layer zu optimieren. Die Modellarchitektur basiert auf einem Deep Deterministic Policy Gradient (DDPG), der mit Konvolutions-Layern und Long Short-Term Memory (LSTM) Netzwerken trainiert wird. Dieses Modell nutzt mehrere Kanäle gleichzeitig, um Abhängigkeiten zu erfassen und zu verarbeiten.

Um die Ressourcennutzung zu optimieren, wird eine parallele Trainingsarchitektur verwendet. Ein zentrales Actor-Critic-Netzwerk lernt aus den Erfahrungen vieler parallel laufender Simulationen, was die Speicherauslastung und die Effizienz der Trainingsprozesse verbessert.

\section{Verwendete Methoden}
In dieser Arbeit werden folgende Methoden und Modelle verwendet:
\begin{itemize}
    \item \textbf{Deep Deterministic Policy Gradient (DDPG)}: Ein algorithmischer Ansatz zur Entscheidungsfindung in kontinuierlichen Aktionsräumen.
    \item \textbf{Konvolutions-Layer}: Spezielle Layer, die es ermöglichen, sehr lange Datenreihen effizient zu verarbeiten.
    \item \textbf{Long Short-Term Memory (LSTM)}: Ein spezieller RNN-Typ zur Verarbeitung und Vorhersage von zeitsequenziellen Daten.
    \item \textbf{Logarithmische Funktionen}: Optimierung der Datenverarbeitung im Konvolutions-Layer.
    \item \textbf{Parallele Architektur}: Nutzung eines zentralen Actor-Critic-Netzwerks, das aus mehreren parallel laufenden Simulationen lernt.
\end{itemize}

\begin{thebibliography}{9}
\bibitem{brunton2019data}
Brunton, S. L., \& Kutz, J. N. (2019). Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control. Cambridge University Press.

\bibitem{Almawlawe2023}
Almawlawe, A., et al. (2023). Applications of Reinforcement Learning in Financial Trading. Journal of Financial Markets.

\end{thebibliography}
