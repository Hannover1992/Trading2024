\subsection{Das Multi-Armed Bandit Problem im Kontext der Degradation}

Das Multi-Armed Bandit Problem, häufig als ``Bandit Problem'' abgekürzt, stellt ein klassisches Dilemma in der Entscheidungsfindung unter Unsicherheit dar. Ursprünglich aus der Glücksspielwelt stammend, bei dem es darum geht, den gewinnbringendsten von mehreren Spielautomaten (den ``Banditen'') auszuwählen, hat dieses Problem vielfältige Anwendungen in den Bereichen künstliche Intelligenz und Maschinenlernen gefunden. Die Herausforderung liegt hierbei im Balancieren zwischen Exploration (Erkunden neuer oder weniger bekannter Aktionen) und Exploitation (Nutzen der besten bekannten Aktionen) \cite{SuttonBarto2018}.

In Bezug auf Degradationsvorgänge wird das Bandit Problem herangezogen, um den kontinuierlichen Verfall von Zuständen zu modellieren. Die Degradation, die sich über eine längere Zeitspanne erstreckt, lässt sich mittels eines markovschen Prozesses darstellen. Charakteristisch hierfür ist, dass trotz Durchführung einer Aktion der Zustandsübergang nur minimal ist, sodass es so wirkt, als würde das System in denselben Zustand zurückkehren.

Der gewählte Ansatz beinhaltet eine Sequenz von Zuständen \(S_1, \ldots, S_N\). In jedem dieser Zustände gibt es mehrere Aktionen, die den jeweiligen PID-Koeffizienten in diesem spezifischen Zustand repräsentieren. Interessanterweise wird durch die Modellierung der markovsche Prozess in ein Multi-Armed Bandit Problem überführt, sodass Zustände von \(S_0, \ldots, S_N\) existieren. Bei jeder Aktion innerhalb eines Zustands, charakterisiert durch drei PID-Parameter, gelangt das System wieder in denselben Zustand \cite{russell2021ai}.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.8cm,
		semithick]
		% Zustände
		\node[state] (Z1)              {\(Z_1\)};
		\node[state] (Z2) [right=of Z1] {\(Z_2\)};
		\node        (Zdots) [right=of Z2] {\(\cdots\)};
		\node[state] (Zn) [right=of Zdots] {\(Z_N\)};

		% Übergänge für Z1
		\path (Z1) edge [loop left] node[left] {$p_1$, $r_1$} 			(Z1)
		(Z1) edge [loop right] node[right] {\(\dots\)} 			(Z1)
		(Z1) edge [loop above] node[above] {$p_2$, $r_2$} 			(Z1)
		(Z1) edge [out=-45, in=-135, looseness=3] node[below] {$p_N$, $r_N$} 	(Z1);

		% Übergänge für Z2
		\path (Z2) edge [loop left] node[left] {$p_1$, $r_1$} 			(Z2)
		(Z2) edge [loop right] node[right] {\(\dots\)} 			(Z2)
		(Z2) edge [loop above] node[above] {$p_2$, $r_2$} 			(Z2)
		(Z2) edge [out=-45, in=-135, looseness=3] node[below] {$p_N$, $r_N$} 	(Z2);

		% Übergänge für ZN
		\path (Zn) edge [loop left] node[left] {$p_1$, $r_1$} 			(Zn)
		(Zn) edge [loop right] node[right] {\(\dots\)} 			(Zn)
		(Zn) edge [loop above] node[above] {$p_2$, $r_2$} 			(Zn)
		(Zn) edge [out=-45, in=-135, looseness=3] node[below] {$p_N$, $r_N$} 	(Zn);

	\end{tikzpicture}
	\caption{Erweiterte Darstellung des Multi-Armed Bandit Problems mit Zustandsaktionen von 1 bis N}
	\label{fig:ExtendedBanditDegradation}
\end{figure}

\paragraph{Legende}
\begin{itemize}
	\item Kanten: Erste Zahl \( p \) symbolisiert die Wahrscheinlichkeit der Aktion \( a \) 
	\item zweite Zahl \( r \) die Belohnung der Aktion.
\end{itemize}
